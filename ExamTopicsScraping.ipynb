{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import os\n",
    "import time\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 721/721        \n",
      "Number of questions found: 1395\n"
     ]
    }
   ],
   "source": [
    "# Base URL\n",
    "url = \"https://www.examtopics.com/discussions/cisco/\"\n",
    "exam_name = \"200-301\"\n",
    "\n",
    "headers = {\n",
    "        \"User-Agent\": (\n",
    "            \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\n",
    "            \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
    "            \"Chrome/122.0.0.0 Safari/537.36\"\n",
    "        ),\n",
    "        \"Accept-Language\": \"en-US,en;q=0.9\",\n",
    "        \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8\",\n",
    "        \"Referer\": \"https://google.com\",\n",
    "        \"Connection\": \"keep-alive\",\n",
    "    }\n",
    "\n",
    "# Get the first page to find number of pages\n",
    "response = requests.get(url, headers=headers)\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "# Find number of pages\n",
    "page_indicator = soup.find(\"span\", class_=\"discussion-list-page-indicator\")\n",
    "strong_tags = page_indicator.find_all(\"strong\")\n",
    "num_pages = int(strong_tags[1].text)\n",
    "\n",
    "question_links = []\n",
    "# Loop through all pages\n",
    "for i in range(1, num_pages + 1):\n",
    "    print(f\"Page {i}/{num_pages}\".ljust(20), end=\"\\r\")\n",
    "    page_url = url + f\"{i}/\"\n",
    "\n",
    "    page_response = requests.get(page_url, headers=headers)\n",
    "    soup = BeautifulSoup(page_response.content, \"html.parser\")\n",
    "    titles = soup.find_all(\"div\", class_=\"dicussion-title-container\")\n",
    "    for title in titles:\n",
    "        if title.text:\n",
    "            title_text = title.text.strip()\n",
    "            if exam_name in title_text:\n",
    "                a_tag = title.find(\"a\")\n",
    "                if a_tag and \"href\" in a_tag.attrs:\n",
    "                    question_links.append(a_tag[\"href\"])\n",
    "\n",
    "print()\n",
    "print(\"Number of questions found:\", len(question_links))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('links.json', 'w') as f:\n",
    "    json.dump(question_links, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_page(link):\n",
    "    question_object = {}\n",
    "\n",
    "    headers = {\n",
    "        \"User-Agent\": (\n",
    "            \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\n",
    "            \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
    "            \"Chrome/122.0.0.0 Safari/537.36\"\n",
    "        ),\n",
    "        \"Accept-Language\": \"en-US,en;q=0.9\",\n",
    "        \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8\",\n",
    "        \"Referer\": \"https://google.com\",\n",
    "        \"Connection\": \"keep-alive\",\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = requests.get(link, headers=headers)\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "    except Exception as e:\n",
    "        return {\n",
    "            \"question\": \"\",\n",
    "            \"answers\": [],\n",
    "            \"comments\": [],\n",
    "            \"most_voted\": None,\n",
    "            \"link\": link,\n",
    "            \"question_number\": \"unknown\",\n",
    "            \"error\": f\"Request or parsing failed: {e}\"\n",
    "        }\n",
    "\n",
    "    question_number_match = re.search(r\"question-(\\d+)\", link)\n",
    "    question_number = question_number_match.group(1) if question_number_match else \"unknown\"\n",
    "\n",
    "    # Extract question\n",
    "    question = \"\"\n",
    "    try:\n",
    "        question_div = soup.find(\"div\", class_=\"question-body\")\n",
    "        question_content = question_div.find(\"p\", class_=\"card-text\") if question_div else None\n",
    "        if question_content:\n",
    "            question = question_content.decode_contents().strip()\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # Extract most voted answers\n",
    "    most_voted = None\n",
    "    try:\n",
    "        voted_answers = soup.find(\"div\", class_=\"voted-answers-tally\")\n",
    "        if voted_answers:\n",
    "            script_content = voted_answers.find(\"script\")\n",
    "            if script_content and script_content.string:\n",
    "                voted_json = json.loads(script_content.string)\n",
    "                most_voted_object = next((item for item in voted_json if item.get('is_most_voted')), None)\n",
    "                if most_voted_object:\n",
    "                    most_voted = most_voted_object.get(\"voted_answers\", None)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # Extract answer options\n",
    "    answers = []\n",
    "    try:\n",
    "        if question_div:\n",
    "            answers_div = question_div.find(\"div\", class_=\"question-choices-container\")\n",
    "            if answers_div:\n",
    "                answer_options = answers_div.find_all(\"li\")\n",
    "                if answer_options:\n",
    "                    answers = [re.sub(r'\\s+', ' ', answer_option.text).strip() for answer_option in answer_options]\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # Extract comments and replies\n",
    "    comments = []\n",
    "    try:\n",
    "        discussion_div = soup.find(\"div\", class_=\"discussion-container\")\n",
    "        comment_divs = discussion_div.find_all(\"div\", class_=\"comment-container\", recursive=False) if discussion_div else []\n",
    "        for comment_div in comment_divs:\n",
    "            comment = {}\n",
    "            try:\n",
    "                comment_content_div = comment_div.find(\"div\", class_=\"comment-content\")\n",
    "                comment_content = comment_content_div.text.strip() if comment_content_div else \"\"\n",
    "            except Exception:\n",
    "                comment_content = \"\"\n",
    "\n",
    "            try:\n",
    "                comment_selected_answer = comment_div.find(\"div\", class_=\"comment-selected-answers\")\n",
    "                selected_answer = comment_selected_answer.find(\"span\").text.strip() if comment_selected_answer else \"\"\n",
    "            except Exception:\n",
    "                selected_answer = \"\"\n",
    "\n",
    "            replies = []\n",
    "            try:\n",
    "                comment_replies_div = comment_div.find(\"div\", class_=\"comment-replies\")\n",
    "                if comment_replies_div:\n",
    "                    reply_divs = comment_replies_div.find_all(\"div\", class_=\"comment-container\")\n",
    "                    for reply in reply_divs:\n",
    "                        try:\n",
    "                            reply_content = reply.find(\"div\", class_=\"comment-content\").text.strip()\n",
    "                        except Exception:\n",
    "                            reply_content = \"\"\n",
    "                        replies.append(reply_content)\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "            comment[\"content\"] = comment_content\n",
    "            comment[\"selected_answer\"] = selected_answer\n",
    "            comment[\"replies\"] = replies\n",
    "\n",
    "            comments.append(comment)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    question_object[\"question\"] = question\n",
    "    question_object[\"answers\"] = answers\n",
    "    question_object[\"comments\"] = comments\n",
    "    question_object[\"question_number\"] = question_number\n",
    "    question_object[\"link\"] = link\n",
    "    question_object[\"most_voted\"] = most_voted\n",
    "    question_object[\"error\"] = None\n",
    "\n",
    "    return question_object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_saved_questions(json_path):\n",
    "    if not os.path.exists(json_path):\n",
    "        return []\n",
    "    with open(json_path, 'r', encoding='utf-8') as f:\n",
    "        try:\n",
    "            return json.load(f)\n",
    "        except json.JSONDecodeError:\n",
    "            return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1395/1395 - Skipping https://www.examtopics.com/discussions/cisco/view/38787-exam-200-301-topic-1-question-39-discussion//n/\r"
     ]
    }
   ],
   "source": [
    "questions = load_saved_questions(\"questions.json\")\n",
    "prefix = \"https://www.examtopics.com\"\n",
    "questions_num = len(question_links)\n",
    "for i, link in enumerate(question_links):\n",
    "    question_number_match = re.search(r\"question-(\\d+)\", link)\n",
    "    question_number = question_number_match.group(1) if question_number_match else \"unknown\"\n",
    "    if question_number in [q[\"question_number\"] for q in questions]:\n",
    "        print(f\"{i+1}/{questions_num} - Skipping {prefix+link}\".ljust(60), end=\"\\r\")\n",
    "        continue\n",
    "    print(f\"{i+1}/{questions_num} - Scraping {prefix+link}\".ljust(60), end=\"\\r\")\n",
    "    question_object = scrape_page(prefix+link)\n",
    "    if question_object[\"error\"]:\n",
    "        print()\n",
    "        print(f\"Error: {question_object['error']}\")\n",
    "        break\n",
    "    questions.append(question_object)\n",
    "    time.sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"questions.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(questions, f, ensure_ascii=False, indent=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
